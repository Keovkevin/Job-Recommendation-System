{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport matplotlib.pyplot as plt\nimport os\nimport nltk\nfrom nltk.corpus import stopwords\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\n#from genism.models import Word2Vec\nprint(os.listdir(\"../input\"))\nprint(os.listdir(\"..\"))\nimport spacy\n#from spacy import displacy\n#from topia.termextract import extract\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df2=pd.read_csv(\"../input/us-technology-jobs-on-dicecom/dice_com-job_us_sample.csv\")\nprint(df2.head())\njobs=[]\nfor job_title  in df2.jobtitle:\n    if(job_title.lower() not in jobs):\n        jobs.append(job_title)\n#print(jobs)\njob_description=np.asarray(df2.loc[:,\"jobdescription\"])\nprint(len(job_description[0:5]))\n#nlp=spacy.load('en')\n\ndef remove_whitespace_entities(doc):\n    doc.ents=[x for x in doc.ents if not (x.text.isspace())]\n    return doc\n#nlp.add_pipe(remove_whitespace_entities,after='ner')\n#article=job_description[2]\n#doc=nlp(article)\n#article=[x for x in nlp(article) if not x.is_stop and x.pos!='PUNCT']\n#article=[x.lemma_ for x in article]\n\n#Using Named Entity Recognition\n# displacy.render(nlp(str(article)), jupyter=True, style='ent')\n# nlp(str(article)).ents\n\n#Comparing similarity of each word with a given set of words. If the similarity score is high, the word is related to technology. Hence, it should be considered.\ndf_languages=pd.read_excel('../input/fr-pl/languages.xlsx')\ndf_frameworks=pd.read_excel(\"../input/fr-pl/frameworks.xlsx\",header=None,error_bad_lines=False,delim_whitespace=True)\nexperience_regex=['\\d+ years','\\d+ experience','']\n#print(df_frameworks)\nframe=[str(x).split(\",\")[0] for x in df_frameworks.iloc[:,0]]\nprint(len(df_frameworks.columns))\ndictionary=list(df_languages.iloc[:,0])\ndictionary.extend(frame)\nprint(dictionary)\ndictionary=[x.lower() for x in dictionary]\nextracted_jobs=dict()\nfor i in range(len(job_description[:10000])):\n    job_id=df2.iloc[i,-1]\n    job=df2.iloc[i,3]\n    flag=0\n    for word in job.split(\" \"):\n        word=word.lower()\n        if word in dictionary:\n            flag=1\n            if job_id not in extracted_jobs.keys():\n                extracted_jobs[job_id]=[]\n            if word not in extracted_jobs[job_id]:\n                extracted_jobs[job_id].append(word)\n    if(flag==0):\n        print(job_id)\nprint(extracted_jobs)\nprint(len(extracted_jobs))        \n            \n#doc_given_text=nlp(u'computer science')\n#sample_word=nlp(u'Java')\n#words=[]\n#for ele in doc:\n#    if(doc_given_text.similarity(ele)>0.5):\n#        words.append(ele.text)\n#print(words)\n#print(doc_given_text.similarity(sample_word))\n#print(doc_given_text.similarity(doc[3]))\n#print(doc[3].vector)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9fd6ded1d0380f736a812be9357860be55f2d597"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c836e37a0973ad100295f29d0a91ac45268babe4"
      },
      "cell_type": "code",
      "source": "df2=pd.read_csv(\"../input/us-technology-jobs-on-dicecom/dice_com-job_us_sample.csv\")\nprint(df2.head())\njobs=[]\nfor job_title  in df2.jobtitle:\n    if(job_title.lower() not in jobs):\n        jobs.append(job_title)\n#print(jobs)\njob_skills=np.asarray(df2.loc[:,\"skills\"])\nprint(len(job_description[0:5]))\n#nlp=spacy.load('en')\n\ndef remove_whitespace_entities(doc):\n    doc.ents=[x for x in doc.ents if not (x.text.isspace())]\n    return doc\n#nlp.add_pipe(remove_whitespace_entities,after='ner')\n#article=job_description[2]\n#doc=nlp(article)\n#article=[x for x in nlp(article) if not x.is_stop and x.pos!='PUNCT']\n#article=[x.lemma_ for x in article]\n\n#Using Named Entity Recognition\n# displacy.render(nlp(str(article)), jupyter=True, style='ent')\n# nlp(str(article)).ents\n\n#Tokenizing words\nextracted_skills=dict()\ntraining_range=int(0.7*len(job_skills))\nfor i in range(training_range):\n    #print(i)\n    #Method 1: Manual pre-processing\n    job_id=df2.iloc[i,-1]\n#     job=df2.iloc[i,-2]\n#     words=job_skills[i].split(\",\")\n#     if(words[0].lower()==\"(see job description)\"):\n#         continue\n#     #print(words)\n#     #print(len(words))\n#     for i in range(len(words)):\n#         #print(type(word))\n#         chunk=words[i].split(\"/\")\n#         words.remove(words[i])\n#         words.extend(chunk)\n            \n#     extracted_skills[job_id]=[]\n#     extracted_skills[job_id].extend(words)\n    #Method 2:Using NLTK\n    tokenizer=nltk.tokenize.RegexpTokenizer(r'\\w+')\n    #print(job_skills[i])\n    if(pd.isnull(job_skills[i])):\n        continue\n    stopwords_list=stopwords.words(\"english\")\n    tokens=re.split(\"|\".join([\",\",\" and\",\"/\",\" AND\",\" or\",\" OR\",\";\"]),job_skills[i])\n    tokens=list(set(tokens))\n    #tokens=[x for x in tokens if x.isalpha()==True]\n    #tokens=tokenizer.tokenize(job_skills[i])\n    #print(tokens)\n    #tokens=nltk.word_tokenize(job_skills[i])\n    #print(stopwords)\n    #stopwords_list=stopwords\n    #words=[x for x in tokens if x not in stopwords_list]\n    #print(tokens)\n    extracted_skills[job_id]=[]\n    extracted_skills[job_id].extend(tokens)\n    #print(extracted_skills[job_id])\nprint(extracted_skills)    \n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c4827ed5e92d47eaa059096e4edea8afcd2a6e06"
      },
      "cell_type": "code",
      "source": "class job_postings:    \n    def __init__(self,link):\n        self.df2=pd.read_csv(link)\n    def clean_skills(self,training_range):\n        extracted_skills=dict()\n        job_skills=np.asarray(self.df2.loc[:,\"skills\"])\n        for i in range(training_range):\n            #print(i)\n            #Method 1: Manual pre-processing\n            job_id=self.df2.iloc[i,-1]\n            #Method 2:Using NLTK\n            tokenizer=nltk.tokenize.RegexpTokenizer(r'\\w+')\n            #print(job_skills[i])\n            if(pd.isnull(job_skills[i])):\n                continue\n            stopwords_list=stopwords.words(\"english\")\n            tokens=re.split(\"|\".join([\",\",\" and\",\"/\",\" AND\",\" or\",\" OR\",\";\"]),job_skills[i])\n            tokens=list(set(tokens))\n            extracted_skills[job_id]=[]\n            extracted_skills[job_id].extend(tokens)\n        return extracted_skills\n    def extract_skills(self,extracted_skills):\n        df_languages=pd.read_excel('../input/fr-pl/languages.xlsx')\n        df_frameworks=pd.read_csv(\"../input/new-fr/frameworks.csv\")\n        df_database=pd.read_csv(\"../input/databse/database.csv\")\n        df_os=pd.read_csv(\"../input/operating-sys/operating_systems.csv\")\n        df_plat=pd.read_csv(\"../input/platform/platforms.csv\")\n        frameworks=df_frameworks.iloc[:,1].tolist()\n        frameworks=[x.lower().strip() for x in frameworks]\n        #frameworks=[str(x).split(\",\")[0] for x in df_frameworks.iloc[:,1]]\n        languages=list(df_languages.iloc[:,0])\n        languages=[x.lower().strip() for x in languages]\n        #frameworks=[x.lower().strip().split('\\t')[0] for x in frameworks]\n        databases=df_database.iloc[:,0].tolist()\n        databases=[x.lower().strip() for x in databases]\n        op_systems=df_os.iloc[:,0].tolist()\n        op_systems=[x.lower().strip() for x in op_systems]\n        platforms=df_plat.iloc[:,1].tolist()\n        #print(platforms)\n        platforms=[x.lower().strip() for x in platforms]\n        #print(frameworks)\n        new_extracted=dict()\n        for ele in extracted_skills.keys():\n            final_lang=''\n            final_frame=''\n            final_others=''\n            final_database=''\n            final_plat=''\n            final_os=''\n            #print(extracted_skills[ele])\n            for skill in extracted_skills[ele]:\n                skill_base=skill.lower().strip()\n                #print(skill_base)\n                if(skill_base in languages):\n                    if(final_lang==''):\n                        final_lang=skill_base\n                    else:\n                        final_lang=final_lang+\",\"+skill_base\n                elif(skill_base in frameworks):\n                    if(final_frame==''):\n                        final_frame=skill_base\n                    else:\n                        final_frame=final_frame+\",\"+skill_base\n                elif(skill_base in databases):\n                    if(final_database==''):\n                        final_database=skill_base\n                    else:\n                        final_database=final_database+\",\"+skill_base\n                elif(skill_base in op_systems):\n                    if(final_os==''):\n                        final_os=skill_base\n                    else:\n                        final_os=final_os+\",\"+skill_base\n                elif(skill_base in platforms):\n                    if(final_plat==''):\n                        final_plat=skill_base\n                    else:\n                        final_plat=final_plat+\",\"+skill_base\n                else:\n                    if(final_others==''):\n                        final_others=skill_base\n                    else:\n                        final_others=final_others+\",\"+skill_base\n            new_extracted[ele]=[final_lang,final_frame,final_database,final_os,final_plat,final_others]\n        extracted_skills_df=pd.DataFrame.from_dict(new_extracted,orient='index',columns=['Language','Framework','Database','OS','Platform','Others'])\n        return extracted_skills_df\n    def create_job_profile(self,extracted_skills_df,domain_df):\n        job_id=extracted_skills_df.index.tolist()\n        languages_df=pd.DataFrame(index=job_id)\n        platforms_df=pd.DataFrame(index=job_id)\n        frameworks_df=pd.DataFrame(index=job_id)\n        databases_df=pd.DataFrame(index=job_id)\n        \n        for job,lang,frame,plat,datab in list(zip(job_id,extracted_skills_df.loc[:,'Language'].tolist(),extracted_skills_df.loc[:,'Framework'].tolist(),extracted_skills_df.loc[:,'Platform'].tolist(),extracted_skills_df.loc[:,'Database'].tolist())):\n            #Languages\n            l=lang.split(\",\")\n            if(lang!=np.nan or lang!=''):\n                for ele in l:\n                    if(ele==''):\n                        continue\n                    if(ele not in languages_df.columns):\n                        #languages.append(ele)\n                        languages_df[ele]=np.nan\n                    languages_df.loc[job,ele]=1\n            \n            #Frameworks\n            l=frame.split(\",\")\n            if(frame!=np.nan or frame!=''):\n                for ele in l:\n                    if(ele==''):\n                        continue\n                    if(ele not in frameworks_df.columns):\n                        #languages.append(ele)\n                        frameworks_df[ele]=np.nan\n                    frameworks_df.loc[job,ele]=1\n\n            #Platforms\n            l=plat.split(\",\")\n            if(plat!=np.nan or plat!=''):\n                for ele in l:\n                    if(ele==''):\n                        continue\n                    if(ele not in platforms_df.columns):\n                        #languages.append(ele)\n                        platforms_df[ele]=np.nan\n                    platforms_df.loc[job,ele]=1\n            \n            #Databases\n            l=datab.split(\",\")\n            if(datab!=np.nan or datab!=''):\n                for ele in l:\n                    if(ele==''):\n                        continue\n                    if(ele not in databases_df.columns):\n                        #languages.append(ele)\n                        databases_df[ele]=np.nan\n                    databases_df.loc[job,ele]=1\n        languages_df=languages_df.reindex_axis(sorted(languages_df.columns), axis=1)\n        frameworks_df=frameworks_df.reindex_axis(sorted(frameworks_df.columns), axis=1)\n        platforms_df=platforms_df.reindex_axis(sorted(platforms_df.columns), axis=1)\n        databases_df=databases_df.reindex_axis(sorted(databases_df.columns), axis=1)\n        domain_df=domain_df.reindex_axis(sorted(domain_df.columns), axis=1)\n\n        languages_df.to_csv(\"languages_job_profile.csv\")\n        frameworks_df.to_csv(\"frameworks_job_profile.csv\")\n        platforms_df.to_csv(\"platforms_job_profile.csv\")\n        databases_df.to_csv(\"databases_job_profile.csv\")\n        domain_df.to_csv(\"domain_job_profile.csv\")\n        print(languages_df.columns)\nobj=job_postings(\"../input/us-technology-jobs-on-dicecom/dice_com-job_us_sample.csv\")\ntraining_range=int(0.7*len(job_skills))\nextracted_skills=obj.clean_skills(training_range)\nextracted_skills_df=obj.extract_skills(extracted_skills)\nprint(extracted_skills_df)\ndomain_df=pd.read_csv(\"../input/domain-df/preprocessed_df.csv\")\nobj.create_job_profile(extracted_skills_df,domain_df)\n#extracted_skills_df.to_csv(\"skill_extracted.csv\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bc10bfb3c641c788952337123b05828afdf19d26"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6ac15adc6a748dc3813694a054a1b2e7909a3ade"
      },
      "cell_type": "markdown",
      "source": "Part A: Building the content analyzer"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4403e95e5d0c3ebea3e178e9b9d03c72403c35e7"
      },
      "cell_type": "code",
      "source": "#Identifying various categories in job postings\n\n#Method1: Applying TF-IDF on the dataset\ncount=0\ndocs=[]\nfor i in range(len(job_description[:100])):\n    #print(job_description[i])\n    if(job_description[i]==np.nan):\n        continue\n    doc=[x for x in job_description[i].split(\" \") if x not in stopwords_list]\n    docs.append(\" \".join(doc))\nprint(len(docs))\nvectorizer=TfidfVectorizer(ngram_range=(1,2),max_df=0.6,max_features=50)\nresponse=vectorizer.fit_transform(docs)\nname_to_index=vectorizer.get_feature_names()\nresponse=response.toarray()\nscores=pd.DataFrame(data=response[:,:],index=range(len(response)),columns=name_to_index)\nprint(scores)\nmax_col_scores={}\n#print(scores.iloc[0,:])\nfor col in range(len(scores.iloc[0,:])):\n    col_score=sum(scores.iloc[:,col])\n    max_col_scores[name_to_index[col]]=col_score\nmax_col_scores=sorted(max_col_scores.items(),reverse=True,key=lambda x:x[1])[:50]\nprint(max_col_scores)\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e3afbe12e20b0df23d430a016bb4bf2bacdcce76"
      },
      "cell_type": "code",
      "source": "#Method 2: Use a separate domain field in the dataset.\n#Use TF-DF on job titles\ndef cluster_job_titles():\n    job_titles=df2.loc[:,'jobtitle'].tolist()\n    #Tokenization   \n    \n    docs=[]\n    for i in range(len(job_titles[:training_range])):\n        #print(job_description[i])\n        if(job_titles[i]==np.nan):\n            continue\n        doc=[x for x in job_description[i].split(\" \") if x not in stopwords_list]\n        docs.append(\" \".join(doc))\n    print(len(docs))\n    vectorizer=TfidfVectorizer(ngram_range=(1,2),max_df=1.0,max_features=50)\n    response=vectorizer.fit_transform(docs)\n    model=KMeans(n_clusters=10,init='k-means++')\n    model.fit(response)\n    labels=model.labels_\n    return labels\n#name_to_index=vectorizer.get_feature_names()\n#response=response.toarray()\n#scores=pd.DataFrame(data=response[:,:],index=range(len(response)),columns=name_to_index)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5ef3577052cd750b7e49cf622164dd8f4ccee4d5"
      },
      "cell_type": "code",
      "source": "# labels=cluster_job_titles()\n# cluster_mapping=dict()\n# for i in range(len(labels)):\n#     label=labels[i]\n#     if(label not in cluster_mapping.keys()):\n#         cluster_mapping[label]=[]\n#     cluster_mapping[label].append(job_titles[i])\n# print(cluster_mapping)\n# print(len(cluster_mapping))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "60298c92af63937bfd75c9b97213a56fe586f777"
      },
      "cell_type": "code",
      "source": "# #Predefined categories\n# #Compare similarities of word embeddings\n# nlp=spacy.load('en_core_web_lg')\n# # jobs=df2.loc[:,'jobtitle'].tolist()[:50]\n# # jobs_descriptions=df2.loc[:,'jobdescription'].tolist()[:50]\n# # for job in jobs:\n# #     #print(job)\n# #     doc=nlp(job)\n# #     print(doc.ents)\n#     #print(\"Entity is\",ele.text,\"Label\",ele.label_)\n# def check_threshold(threshold,ele):\n#     if(ele[0]!=threshold[0][0] and abs(ele[1]-threshold[0][1])<0.03 and ele[1]>0.5):\n#         return True\n#     else:\n#         return False\n# def categorize(training_range):\n#     job_id=df2.loc[:,'uniq_id'].tolist()[:training_range]\n#     job_titles=df2.loc[:,'jobtitle'].tolist()[:training_range]\n#     job_descriptions=df2.loc[:,'jobdescription'].tolist()[:training_range]\n#     final_cat=pd.DataFrame(index=job_id)\n#     #categories=['Network Engineer','Application Development','Big Data','Data Analyst','Software Developer','DevOps','Software Testing','Front End','Back End','Full Stack','Web Development','Information Security','Mobile developer','System Administrator','Business Analyst','Manager','Cloud']\n#     categories=['Network Engineer','Full stack','QA/Test Developer','Enterprise application','DevOps','Mobile Developer','Back End','Database Administrator(DBA)','Front End','Game developer','System Administrator','Data Scientist','Business analyst','Sales professional','Product Manager','Information Security','Software Developer/Java Developer','Web Developer']\n#     for category in categories:\n#         final_cat[category]=np.nan\n#     for job_t_d in list(zip(job_id,job_titles,job_descriptions)):\n#         id_job=job_t_d[0]\n#         job_i=job_t_d[1]\n#         job_d=job_t_d[2]\n#         job_title=nlp(job_i.lower())\n#         job_description=nlp(job_d.lower())\n#         match_cat_title=dict()\n#         match_cat_description=dict()\n#         for category in categories:\n#             word=nlp(category.lower())\n#             match_cat_title[category]=job_title.similarity(word)\n#             match_cat_description[category]=job_description.similarity(word)\n#         match_cat_title=sorted(match_cat_title.items(),key=lambda x:x[1],reverse=True)\n#         match_cat_description=sorted(match_cat_description.items(),key=lambda x:x[1],reverse=True)\n        \n        \n#         #a represents max\n#         if(match_cat_title[0][1]>0.5 or match_cat_description[0][1]>0.5):\n#             a=match_cat_title[0]\n#             print(a)\n#             match_cat_description=list(filter(lambda x: check_threshold(match_cat_title,x),match_cat_description))\n#             if(len(match_cat_description)!=0):\n#                 print(match_cat_description)\n#                 print(id_job)\n#                 #b=match_cat_description[0]\n#                 final_cat.loc[id_job,a[0]]=1\n#                 match_cat_description.extend([(match_cat_title[0][0],1)])\n#                 sum_proportion=sum([x[1] for x in match_cat_description])\n#                 for ele in match_cat_description:\n#                     final_cat.loc[id_job,ele[0]]=ele[1]/sum_proportion\n#             else:\n#                 print(id_job)\n#                 final_cat.loc[id_job,a[0]]=1\n\n#         else:\n#             print(\"not considering\",job_i)\n#         #print(match_cat)\n#     return final_cat\n# training_range=int(0.7*len(df2.loc[:,'uniq_id']))\n# final_cat=categorize(training_range)\n\n# print(final_cat)\n# final_cat.to_csv(\"preprocessed_df.csv\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1e7658ea046e4f5876928a63aaeb9cebdb0bafbf"
      },
      "cell_type": "code",
      "source": "#Extracting years of experience\nnlp=spacy.load('en_core_web_lg')\njob_description=df2.loc[:,'jobdescription'].tolist()\nid_job=df2.loc[:,'uniq_id'].tolist()\nexperience_regex=['\\d+ years \\w+ $\\.',r'\\d+ experience']\nmatches=dict()\nentities=dict()\nfor job_id,description in list(zip(id_job,job_description))[:10]:\n    #l=re.findall(r\"\\w* experience[\\S*\\s*]\\w*[.]\",description)\n    l=re.findall(r\"[^.]*experience[^.]*\\.\",description)\n    matches[job_id]=l    \n    for string in matches[job_id]:\n        print(string)\n        doc=nlp(string)\n        \n        for token in doc:\n            print(token.text,token.dep_,token.head.text)\nprint(matches)\n#print(entities)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dc1c4c8814ec65c881476d2f8d83fa0e2319d74a"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5b24c1a19e23e3bde4d92103a08a65420bca1d14"
      },
      "cell_type": "code",
      "source": "#Identifying ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ca2a373b2d99422ee8ba00f77883d3446dafa9c7"
      },
      "cell_type": "markdown",
      "source": "Part B: Profile Learner"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0cb0b32b64691045907a06bbe2b0a34f92ea8ff9"
      },
      "cell_type": "code",
      "source": "#Explicit/Implicit ways of identifying user preferences\n#1) Use a like/dislike method to indicate user preference:\n#Optimal method: Model based user-preference method to infer a score for each job(item) that the user has worked for in the past ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}